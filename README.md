
# B5W6: Intelligent Complaint Analysis â€” Week 6 Challenge | 10 Academy

## ğŸ—‚ Challenge Context

This repository documents the submission for 10 Academyâ€™s **B5W6: Intelligent Complaint Analysis** challenge.

CrediTrust Financial, a fast-growing digital finance company operating across East Africa, faces challenges in identifying, understanding, and acting on large volumes of unstructured customer complaints. This project builds an AI-driven Retrieval-Augmented Generation (RAG) pipeline to:

- Transform unstructured complaint narratives into actionable business insights
- Empower non-technical teams to ask questions and receive grounded, evidence-based answers
- Shift the organization from reactive to proactive customer issue resolution

This project includes:
- ğŸ§¹ Clean ingestion and processing of real-world financial complaints data
- ğŸ“Š Exploratory Data Analysis (EDA) of complaint volumes, narratives, and product distributions
- ğŸ” Text chunking and semantic embedding for efficient vector search
- ğŸ§  RAG pipeline combining FAISS-based retrieval with LLM summarization
- ğŸŒ Interactive Streamlit chatbot for internal teams (Product, Compliance, Support)

---

## ğŸ”§ Project Setup

1. Clone the repository:

```bash
git clone https://github.com/NabloP/b5-w6-intelligent-complaint-analysis-challenge.git
cd b5-w6-intelligent-complaint-analysis-challenge
```

2. Create and activate the virtual environment:

**On Windows:**
```bash
python -m venv complaint-analysis-challenge
.\complaint-analysis-challenge\Scripts\Activate.ps1
```

**On macOS/Linux:**
```bash
python3 -m venv complaint-analysis-challenge
source complaint-analysis-challenge/bin/activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ CI/CD (GitHub Actions)

This project uses GitHub Actions for Continuous Integration. On every `push` or `pull_request`, the following checks run:

- Repository checkout
- Python 3.10 setup
- Install and validate dependencies

CI workflow is defined in:

    .github/workflows/unittests.yml

---

## ğŸ” Complaint Analysis Business Understanding

### 1. The Need for Automated Complaint Analysis

With thousands of complaints per month across five key financial products (Credit Cards, Personal Loans, BNPL, Savings, Transfers), internal teams at CrediTrust struggle with:
- Slow manual complaint review
- Lack of systematic insights
- Reactive risk management

An AI-powered complaint analysis system enables:
- Faster identification of top complaint themes
- Early warning for compliance breaches or fraud
- Evidence-backed decision making

### 2. RAG for Strategic Insights

RAG systems combine the power of:
- **Retrieval**: Using vector search (FAISS) to find the most relevant complaint narratives
- **Augmentation**: Feeding retrieved context into a Large Language Model
- **Generation**: Producing a concise, context-grounded answer

This architecture ensures answers are both insightful and verifiable.

---

## ğŸ— Project Components Completed (Tasks 1 & 2)

### âœ… Clean Ingestion & Exploratory Data Analysis (Task 1)

We ingested and analyzed over **650,000 consumer complaints** from the **Consumer Financial Protection Bureau (CFPB)**, performing:
- ğŸ“Š **Complaint Volume Analysis** by product, time, and narrative length
- ğŸ§¹ **Schema Audit & Missingness Diagnostics** to ensure only business-relevant, high-integrity fields were retained
- ğŸ“ **Text Preprocessing** using a **lossless cleaning pipeline** (`src/chunking/text_cleaner.py`) to preserve linguistic nuances crucial for semantic search

Key Output:  
`data/interim/filtered_complaints.csv`  
Containing ~270,000 cleaned and filtered complaint narratives across **five strategic financial products**:  
Credit Cards, Personal Loans, Buy Now Pay Later (BNPL), Savings Accounts, and Money Transfers.

---

### âœ… Text Chunking, Embedding & ChromaDB Indexing (Task 2)

To prepare complaint narratives for efficient semantic search, we built a **modular chunking and embedding pipeline** using:

| Component | Implementation Details |
|-----------|------------------------|
| **Chunking Logic** | Used **LangChainâ€™s RecursiveCharacterTextSplitter** (`src/chunking/text_chunker.py`) with:<br> â€¢ Chunk Size: **500 tokens** <br> â€¢ Overlap: **50 tokens** <br> This preserves context while staying within embedding model limits. |
| **Embedding Model** | Chose **all-MiniLM-L6-v2** for:<br> â€¢ âš¡ **Speed**: Fast inference on CPU<br> â€¢ ğŸ¯ **Accuracy**: Strong semantic matching performance in general language and complaint-style text |
| **Vector Store** | Created a **ChromaDB** vector store (`src/chunking/vector_store_builder.py`), storing:<br> â€¢ Embeddings<br> â€¢ Associated metadata (Product, Date, Complaint ID, Raw Text)<br> â€¢ Persisted under `/vector_store/` for fast, reusable semantic retrieval |

We opted for **ChromaDB** over FAISS to take advantage of:
- **Native metadata storage**
- **Lightweight integration with LangChain**
- **Ease of deployment in production workflows**

Key Script:  
`scripts/embedding_runner.py`  
Allows end-to-end execution of chunking, embedding, and vector index creation in a single command.

---

## ğŸ”— Key Technology Decisions & Justifications

| Decision | Rationale |
|----------|-----------|
| âœ… **ChromaDB** over FAISS | Better metadata handling, easier integration with LangChain for future RAG deployment |
| âœ… **all-MiniLM-L6-v2** | Optimal trade-off between **semantic precision** and **computational efficiency**â€”ideal for CrediTrustâ€™s real-time requirements |
| âœ… **Recursive Chunking** | Ensures **no context loss** for long-form complaints while enabling short narratives to pass unaltered |

---

<!-- TREE START -->
ğŸ“ Project Structure

solar-challenge-week1/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ ui_helpers.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ interim/
â”‚   â”‚   â”œâ”€â”€ filtered_complaints.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ complaints.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ task-1-eda-preprocessing.ipynb
â”‚   â”œâ”€â”€ task-2-embedding-indexing.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ embedding_runner.py
â”‚   â”œâ”€â”€ generate_tree.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ chunking/
â”‚   â”‚   â”œâ”€â”€ embedding_generator.py
â”‚   â”‚   â”œâ”€â”€ text_chunker.py
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â”‚   â”œâ”€â”€ vector_store_builder.py
â”‚   â””â”€â”€ eda/
â”‚       â”œâ”€â”€ eda_visualizer.py
â”‚       â”œâ”€â”€ schema_auditor.py
â””â”€â”€ vector_store/
<!-- TREE END -->
---

## âœ… Interim Status (as of July 6)

| Task # | Task Description | Status | Key Deliverables |
|--------|------------------|--------|------------------|
| **1** | Data Exploration & Cleaning | âœ… Completed | Cleaned dataset, EDA notebooks, schema diagnostics |
| **2** | Chunking & Embedding with ChromaDB | âœ… Completed | Modular scripts, persisted vector store |
| **3** | RAG Pipeline & Evaluation | ğŸ”µ In Progress | Retriever + Generator logic under development |
| **4** | Interactive Streamlit Chatbot | ğŸ”µ Pending | Planned for Task 4 |

---

## ğŸ“Š Task Progress Tracker

| Task # | Task Name                         | Status      | Description |
|--------|------------------------------------|-------------|-------------|
| 1      | Exploratory Data Analysis (EDA)    | âœ… Completed | Visualized complaint volumes, lengths, nulls; filtered data for target products. |
| 2      | Text Chunking & Embedding          | âœ… Completed | Applied RecursiveCharacterTextSplitter, generated embeddings, stored with FAISS. |
| 3      | RAG Pipeline Core Logic            | ğŸ”µ In Progress | Building retrieval + generation logic and qualitative evaluation. |
| 4      | Interactive Streamlit Interface    | ğŸ”µ Pending | Streamlit chatbot with source transparency and real-time querying. |

---

## ğŸ” Next Steps

1. Build the **Retriever + LLM Prompt** pipeline using precomputed embeddings.
2. Conduct **qualitative evaluation** using a curated question bank.
3. Deploy an **interactive Streamlit app** for CrediTrustâ€™s internal teams.
4. Implement **explainability layers** to enhance trust and regulatory compliance.

---

## ğŸš€ Planned Final Deliverables

| Deliverable | Format / Location |
|------------|-------------------|
| Cleaned Complaint Dataset | `data/interim/filtered_complaints.csv` |
| EDA & Visuals | `notebooks/task-1-eda-preprocessing.ipynb` |
| Embedding & Indexing Pipeline | `scripts/embedding_runner.py` |
| ChromaDB Vector Store | `/vector_store/` |
| RAG Pipeline | `src/rag_pipeline.py` |
| Streamlit Chatbot | `app/app.py` |

---

## ğŸ“š References

- LangChain Documentation
- ChromaDB
- Hugging Face Sentence Transformers
- Streamlit
- CFPB Open Data

---

## ğŸ‘¤ Author

**Nabil Mohamed**  
10 Academy AIM Bootcamp Participant  
GitHub: [@NabloP](https://github.com/NabloP)